{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T09:24:43.348640Z",
     "iopub.status.busy": "2025-07-19T09:24:43.348147Z",
     "iopub.status.idle": "2025-07-19T09:24:58.516218Z",
     "shell.execute_reply": "2025-07-19T09:24:58.515523Z",
     "shell.execute_reply.started": "2025-07-19T09:24:43.348616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ACER\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ACER\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ACER\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LANGKAH 1: INSTALASI PAKET WAJIB DI SETIAP SESI\n",
    "# Blok ini harus ada di paling atas dan dijalankan setiap kali sesi dimulai.\n",
    "# =============================================================================\n",
    "!pip install -q scikit-learn==1.3.2\n",
    "!pip install -q imbalanced-learn==0.11.0\n",
    "!pip install -q Sastrawi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T09:24:58.518141Z",
     "iopub.status.busy": "2025-07-19T09:24:58.517915Z",
     "iopub.status.idle": "2025-07-19T09:26:40.870346Z",
     "shell.execute_reply": "2025-07-19T09:26:40.869692Z",
     "shell.execute_reply.started": "2025-07-19T09:24:58.518119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01memoji\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANALISIS SENTIMEN JSS (V21 - VERSI STABIL DENGAN PELABELAN BENAR)\n",
    "# =============================================================================\n",
    "\n",
    "# SECTION 1: IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Konfigurasi awal\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# SECTION 2: TEXT PREPROCESSING CLASS\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = {\n",
    "            'yang', 'dan', 'di', 'ke', 'dari', 'untuk', 'dengan', 'pada', 'dalam', 'atau', 'ini', 'itu', 'juga', 'akan',\n",
    "            'sudah', 'telah', 'adalah', 'ada', 'pernah', 'sedang', 'masih', 'sering', 'selalu', 'kadang', 'biasanya', 'mungkin', \n",
    "            'segera', 'seperti', 'karena', 'kalau', 'jika', 'tapi', 'tetapi', 'namun', 'pas', 'serta', 'agar', 'supaya',\n",
    "            'oleh', 'sehingga', 'tersebut', 'yakni', 'yaitu', 'pula', 'saya', 'kamu', 'dia', 'mereka', 'kami', 'kita', \n",
    "            'ku', 'mu', 'pun', 'kah', 'lah', 'harus', 'buka', 'daftar', 'tolong', 'login', 'masuk', 'pilih', 'kirim', \n",
    "            'isi', 'pakai', 'akses', 'lihat', 'liat', 'buat', 'mohon', 'banget', 'sekali', 'aja', 'saja', 'saat', 'waktu',\n",
    "            'aplikasi', 'program', 'sistem', 'website', 'fitur', 'menu', 'akun', 'data', 'layanan', 'lapor', 'aduan', \n",
    "            'keluhan', 'informasi', 'info', 'verifikasi', 'update', 'jss', 'yogyakarta', 'masyarakat', 'min', 'admin',\n",
    "            'notifikasi', 'notif', 'email', 'upload', 'file', 'password', 'username', 'nomor', 'telepon', 'otp', 'nik', \n",
    "            'saran', 'solusi', 'bintang', 'wae', 'dab', 'lurr', 'sih', 'dong', 'deh', 'kok', 'koq', 'mah', 'nya', \n",
    "            'lho', 'toh', 'gais', 'kak', 'gan', 'bro', 'halo', 'hallo', 'hai', 'wah', 'apa', 'kenapa', 'bagaimana', \n",
    "            'kapan', 'dimana', 'kpd', 'dll', 'dsb', 'yth', 'cctv', 'tv', 'hotspot','maps','chat','tema','ndes','nggih'\n",
    "        }\n",
    "        self.slang_word_dict = {\n",
    "            'eroorrr': 'error', 'eror': 'error', 'blom': 'belum', 'blm': 'belum', 'durung': 'belum', 'ndak': 'tidak', \n",
    "            'ga': 'tidak', 'gak': 'tidak', 'nggak': 'tidak', 'tdk': 'tidak', 'g': 'tidak', 'gk': 'tidak', 'ra': 'tidak',\n",
    "            'mboten': 'tidak', 'gaada': 'tidak ada', 'gabisa': 'tidak bisa', 'gakbisa': 'tidak bisa', 'bgt': 'banget', \n",
    "            'dgn': 'dengan', 'dg': 'dengan', 'krn': 'karena', 'utk': 'untuk', 'sdh': 'sudah', 'udh': 'sudah', 'dah': 'sudah',\n",
    "            'wes': 'sudah', 'sy': 'saya', 'dr': 'dari', 'seko': 'dari', 'klo': 'kalau', 'kalo': 'kalau', 'skrg': 'sekarang', \n",
    "            'lg': 'lagi', 'bnyk': 'banyak', 'byk': 'banyak', 'jg': 'juga', 'bkn': 'bukan', 'td': 'tadi', 'trs': 'terus', \n",
    "            'trus': 'terus', 'gmn': 'bagaimana', 'gimana': 'bagaimana', 'pripun': 'bagaimana', 'kpn': 'kapan', 'tgl': 'tanggal', \n",
    "            'thn': 'tahun', 'bln': 'bulan', 'kpd': 'kepada', 'ttp': 'tetap', 'sbg': 'sebagai', 'pdhl': 'padahal', 'great': 'bagus',\n",
    "            'bbrp': 'beberapa', 'hp': 'ponsel', 'tp': 'tapi', 'gpp': 'tidak apa-apa', 'sm': 'sama', 'lgsg': 'langsung', 'muter2': 'lambat',\n",
    "            'cmn': 'cuma', 'cuman': 'hanya', 'app': 'aplikasi', 'apps': 'aplikasi', 'knp': 'kenapa', 'msh': 'masih', 'jozz': 'bagus',\n",
    "            'tbtb': 'tiba-tiba', 'moga2': 'semoga', 'mhin': 'mohon', 'nomer': 'nomor', 'pasword': 'password', 'tlp': 'telepon',\n",
    "            'responnya': 'respons', 'lemot': 'lambat', 'ngelag': 'lambat', 'mubeng': 'lambat', 'muter-muter': 'lambat', \n",
    "            'muter': 'lambat', 'crash': 'error', 'force close': 'error tutup paksa', 'fc': 'error tutup paksa', 'loading': 'memuat', \n",
    "            'instal': 'pasang', 'uninstal': 'copot pemasangan', 'register': 'daftar', 'upgrade': 'tingkatkan', 'pemkot': 'pemerintah kota', \n",
    "            'pemda': 'pemerintah daerah', 'dindukcapil': 'dinas kependudukan pencatatan sipil', 'capil': 'pencatatan sipil', 'ktp': 'kartu tanda penduduk', \n",
    "            'kk': 'kartu keluarga', 'akte': 'akta', 'npwp': 'nomor pokok wajib pajak', 'opd': 'organisasi perangkat daerah', 'jogja': 'yogyakarta', \n",
    "            'yogya': 'yogyakarta', 'dishub': 'dinas perhubungan', 'dinkes': 'dinas kesehatan', 'suwun': 'terima kasih', 'okokok': 'oke',\n",
    "            'matur nuwun': 'terima kasih', 'maturnuwun': 'terima kasih', 'nuwun': 'terima kasih', 'ngeten': 'begini', 'lelet': 'lambat',\n",
    "            'angel': 'sulit', 'mumet': 'pusing', 'kesuwen': 'terlalu lama', 'suwe': 'lama', 'jan': 'sungguh', 'tenan': 'sungguh', \n",
    "            'malah': 'justru', 'iki': 'ini', 'opo': 'apa', 'iso': 'bisa', 'gawe': 'buat', 'ndelok': 'lihat', 'mantap': 'bagus', \n",
    "            'keren': 'bagus', 'good': 'bagus', 'nice': 'bagus', 'ok': 'oke', 'oke': 'oke', 'jos': 'bagus', 'joss': 'bagus', \n",
    "            'josss': 'bagus', 'mantab': 'bagus', 'mantuulll': 'bagus', 'mantabbb': 'bagus', 'apik': 'bagus', 'sae': 'bagus', 'istimewaa': 'bagus',\n",
    "            'sip': 'bagus', 'siip': 'bagus', 'sippppppppppp': 'bagus', 'tjakep': 'bagus', 'mantul': 'bagus', 'istimewa': 'bagus', \n",
    "            'top': 'bagus', 'the best': 'terbaik', 'best': 'terbaik', 'dabest': 'terbaik', 'luarbiasa': 'luar biasa', 'force close': 'error',\n",
    "            'bermanfaat': 'manfaat', 'membantu': 'bantu', 'helpful': 'bantu', 'sangat bantu': 'bantu', 'sangat membantu': 'bantu',\n",
    "            'susah': 'sulit', 'dpersulit': 'sulit', 'ribet': 'sulit', 'bertele-tele': 'sulit', 'berbelit': 'sulit', 'payah': 'buruk', \n",
    "            'ancur': 'hancur', 'parah': 'sangat buruk', 'elek': 'jelek', 'bosok': 'busuk', 'gajelas': 'tidak jelas', 'ngak': 'tidak',\n",
    "            'ga jelas': 'tidak jelas', 'zonk': 'gagal', 'thanks': 'terima kasih', 'makasih': 'terima kasih', 'mksh': 'terima kasih', \n",
    "            'trims': 'terima kasih', 'tks': 'terima kasih', 'trimakasih': 'terima kasih', 'diperbaiki': 'perbaiki', 'ditingkatkan': 'tingkat',\n",
    "            'ookkee' : 'oke', 'oke oke': 'oke', 'marai': 'bikin', 'mubeng minger': 'lambat','pekok': 'bodoh','kliteh': 'kriminalitas', \n",
    "            'rong': 'belum', 'nyobo': 'coba','aing': 'saya','bolan-baleni': 'berulang kali','nglebokke': 'memasukkan','bagussss': 'bagus',\n",
    "            'jempoooooooollllll': 'bagus', 'jempooll': 'bagus', 'verygood': 'sangat bagus', 'goodluck': 'semoga berhasil', \"klk\": \"kalo\",\n",
    "            'quick respon': 'respons cepat', 'optimal': 'maksimal', 'baguss': 'bagus', \"joshh\": 'bagus', \"dadi\": \"jadi\", 'b aja': 'biasa saja'\n",
    "        }\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.create_stemmer()\n",
    "\n",
    "    def normalize_elongated_words(self, text):\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    def convert_emojis_to_words(self, text):\n",
    "        emoji_mapping = {\n",
    "            '👍': ' bagus ', '❤️': ' suka ', '😊': ' senang ', '👌': ' oke ', '✅': ' setuju ', '✨': ' bagus sekali ', '⭐': ' bintang ', '💯': ' sempurna ', '👏': ' bagus ', '🙏': ' terima kasih ',\n",
    "            '👎': ' jelek ', '😡': ' marah ', '😠': ' marah ', '😤': ' kesal ', '😭': ' kecewa menangis ', '😢': ' sedih ', '😞': ' kecewa ', '💔': ' kecewa ', '💩': ' jelek sekali ', '❌': ' salah '\n",
    "        }\n",
    "        for emoji_char, meaning in emoji_mapping.items():\n",
    "            text = text.replace(emoji_char, meaning)\n",
    "        return emoji.demojize(text, delimiters=(\" \", \" \")).replace(\"_\", \" \")\n",
    "        \n",
    "    def _handle_negation(self, text):\n",
    "        negation_words = {'tidak', 'bukan', 'jangan', 'kurang', 'belum'}\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            if words[i] in negation_words and i + 1 < len(words):\n",
    "                processed_words.append(words[i] + '_' + words[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "        return ' '.join(processed_words)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = self.convert_emojis_to_words(text)\n",
    "        text = self.normalize_elongated_words(text)\n",
    "    \n",
    "        # --- PERBAIKAN URUTAN ---\n",
    "        # 1. Hapus URL dan Tanda Baca DULU agar teks bersih\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', ' ', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s_]', ' ', text)\n",
    "        text = ' '.join(text.split())\n",
    "    \n",
    "        # 2. BARU lakukan normalisasi slang pada kata-kata yang sudah bersih\n",
    "        words = text.split()\n",
    "        normalized_words = [self.slang_word_dict.get(word, word) for word in words]\n",
    "        text = ' '.join(normalized_words)\n",
    "        \n",
    "        # 3. Lanjutkan dengan stemming, negasi, dan stopword\n",
    "        text = self.stemmer.stem(text)\n",
    "        text = self._handle_negation(text)\n",
    "        \n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# SECTION 2.5: FUNGSI PELABELAN OTOMATIS\n",
    "def label_review_enhanced(row):\n",
    "    review_text = str(row['Ulasan']).lower()\n",
    "    rating = row['Rating']\n",
    "    negative_keywords = ['kecewa', 'sulit', 'lambat', 'error', 'tidak bisa', 'berbelit', 'susah', 'buruk', 'parah', 'gagal', 'eror', 'masalah', 'kendala', 'tidak muncul', 'ribet', 'pusing', 'jelek', 'busuk',]\n",
    "    if rating <= 2: return 'negatif'\n",
    "    if rating == 5: return 'positif'\n",
    "    if rating == 4:\n",
    "        return 'negatif' if any(keyword in review_text for keyword in negative_keywords) else 'positif'\n",
    "    if rating == 3:\n",
    "        if any(keyword in review_text for keyword in negative_keywords): return 'negatif'\n",
    "        return 'netral'\n",
    "    return 'netral'\n",
    "\n",
    "# SECTION 2.6: FUNGSI ANALISIS DATA-DRIVEN\n",
    "def analyze_word_sentiment_distribution(df, threshold=0.70):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔎 ANALISIS DISTRIBUSI SENTIMEN UNTUK ULASAN SATU KATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    single_word_reviews = df[df['word_count_cleaned'] == 1]\n",
    "    \n",
    "    if single_word_reviews.empty:\n",
    "        print(\"Tidak ditemukan ulasan satu kata untuk dianalisis.\")\n",
    "        return set()\n",
    "\n",
    "    distribution = single_word_reviews.groupby('cleaned_text')['sentimen'].value_counts(normalize=True)\n",
    "    \n",
    "    dist_df = distribution.unstack(level=-1).fillna(0)\n",
    "    dist_df['count'] = single_word_reviews['cleaned_text'].value_counts()\n",
    "    dist_df = dist_df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    print(\"Menampilkan distribusi untuk kata yang muncul lebih dari 2 kali:\")\n",
    "    print(dist_df[dist_df['count'] > 2].to_string(float_format=\"{:.2f}\".format))\n",
    "    \n",
    "    strong_sentiment_words = set()\n",
    "    for word, row in dist_df.iterrows():\n",
    "        is_strong_positive = row.get('positif', 0) > threshold\n",
    "        is_strong_negative = row.get('negatif', 0) > threshold\n",
    "        \n",
    "        if is_strong_positive or is_strong_negative:\n",
    "            strong_sentiment_words.add(word)\n",
    "            \n",
    "    print(f\"\\n✅ Ditemukan {len(strong_sentiment_words)} kata sentimen kuat (threshold > {threshold*100}%)\")\n",
    "    print(f\"   Kata-kata tersebut: {sorted(list(strong_sentiment_words))}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return strong_sentiment_words\n",
    "\n",
    "# SECTION 3: KELAS UTAMA SENTIMENT ANALYZER\n",
    "class SentimentAnalyzerLR:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.pipeline = None\n",
    "        self.is_trained = False\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, sublinear_tf=True)),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=2000, class_weight='balanced'))\n",
    "        ])\n",
    "\n",
    "    def _analyze_error_reason(self, cleaned_text):\n",
    "        pos_keywords = {'bagus', 'bantu', 'baik', 'puas', 'keren', 'hebat', 'manfaat', 'suka', 'senang', 'tarik'}\n",
    "        neg_keywords = {'jelek', 'sulit', 'kecewa', 'lambat', 'error', 'buruk', 'parah', 'gagal', 'pusing'}\n",
    "        words = set(cleaned_text.split())\n",
    "        has_pos = any(word in pos_keywords for word in words)\n",
    "        has_neg = any(word in neg_keywords for word in words)\n",
    "        if has_pos and has_neg: return \"Sinyal Sentimen Bertentangan\"\n",
    "        if any('_' in word for word in words): return \"Konteks Negasi/Penguat Sulit\"\n",
    "        if not has_pos and not has_neg: return \"Tidak Ada Kata Kunci Kuat\"\n",
    "        return \"Lain-lain (Konteks/Sarkasme)\"\n",
    "\n",
    "    def train_and_evaluate(self, df_full, text_column='Ulasan', target_column='sentimen'):\n",
    "        if 'cleaned_text' not in df_full.columns:\n",
    "            df_full['cleaned_text'] = df_full[text_column].apply(self.preprocessor.preprocess_text)\n",
    "        else:\n",
    "            print(\"\\n✔️ Kolom 'cleaned_text' sudah ada, melewati langkah preprocessing.\")\n",
    "        \n",
    "        # Filter out rows with empty cleaned_text before splitting\n",
    "        df_full = df_full[df_full['cleaned_text'].str.strip() != ''].copy()\n",
    "\n",
    "        df_train, df_test = train_test_split(df_full, test_size=0.25, random_state=42, stratify=df_full[target_column])\n",
    "        print(f\"\\nPartitioning Data: {len(df_train)} untuk training, {len(df_test)} untuk evaluasi.\")\n",
    "\n",
    "        print(\"\\n🚀 Memulai Training Model dengan Pencarian Parameter Mendalam...\")\n",
    "        X_train, y_train = df_train['cleaned_text'], df_train[target_column]\n",
    "        \n",
    "        param_grid = {\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "            'tfidf__min_df': [2, 3],\n",
    "            'classifier__C': [1, 10, 50, 100]\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(self.pipeline, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.pipeline = grid_search.best_estimator_\n",
    "        self.is_trained = True\n",
    "        print(f\"✅ Training Selesai! Parameter terbaik: {grid_search.best_params_}\")\n",
    "\n",
    "        print(\"\\n📈 HASIL EVALUASI MODEL:\")\n",
    "        X_test, y_test = df_test['cleaned_text'], df_test[target_column]\n",
    "        y_pred = self.pipeline.predict(X_test)\n",
    "        \n",
    "        report_text = classification_report(y_test, y_pred, zero_division=0)\n",
    "        print(report_text)\n",
    "        \n",
    "        self.save_results(report_text, grid_search)\n",
    "        self.plot_and_save_confusion_matrix(y_test, y_pred)\n",
    "        self.show_misclassified_data(df_test, y_pred)\n",
    "        self.show_top_features()\n",
    "\n",
    "    def save_results(self, report_text, grid_search):\n",
    "        print(\"\\n💾 Menyimpan hasil lengkap Grid Search...\")\n",
    "        results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "        results_df.sort_values(by='rank_test_score', inplace=True)\n",
    "        results_df.to_csv('grid_search_results.csv', index=False)\n",
    "        print(\"   -> Hasil disimpan ke 'grid_search_results.csv'\")\n",
    "        print(\"\\n💾 Menyimpan laporan klasifikasi...\")\n",
    "        with open('classification_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*50 + \"\\nLaporan Klasifikasi\\n\" + \"=\"*50 + \"\\n\" + report_text)\n",
    "        print(\"   -> Laporan disimpan ke 'classification_report.txt'\")\n",
    "\n",
    "    def plot_and_save_confusion_matrix(self, y_true, y_pred):\n",
    "        print(\"\\n🖼️  Membuat, menyimpan, dan menampilkan confusion matrix...\")\n",
    "        labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix', fontsize=16)\n",
    "        plt.xlabel('Prediksi', fontsize=12)\n",
    "        plt.ylabel('Aktual', fontsize=12)\n",
    "        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"   -> Confusion matrix berhasil disimpan ke 'confusion_matrix.png'\")\n",
    "        plt.show()\n",
    "\n",
    "    def show_misclassified_data(self, df_test, y_pred):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🕵️  ANALISIS KESALAHAN KLASIFIKASI (MISCLASSIFIED DATA)\")\n",
    "        print(\"=\"*80)\n",
    "        misclassified_mask = df_test['sentimen'] != y_pred\n",
    "        if not misclassified_mask.any():\n",
    "            print(\"✅ Tidak ada data yang salah diklasifikasikan. Model sempurna!\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "            return\n",
    "        misclassified_data = df_test[misclassified_mask].copy()\n",
    "        misclassified_data['prediksi'] = y_pred[misclassified_mask]\n",
    "        misclassified_data['analisis'] = misclassified_data['cleaned_text'].apply(self._analyze_error_reason)\n",
    "        def print_group(title, df_group):\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(f\"📄 {title.upper()} (Total: {len(df_group)})\")\n",
    "            print(\"-\"*80)\n",
    "            if df_group.empty:\n",
    "                print(\"Tidak ada kesalahan untuk kategori ini.\")\n",
    "                return\n",
    "            for index, row in df_group.head(10).iterrows():\n",
    "                print(f\"   Teks Asli        : {row['Ulasan']}\")\n",
    "                print(f\"   Teks Olahan      : {row['cleaned_text']}\")\n",
    "                print(f\"   Label Seharusnya : {row['sentimen'].upper()}\")\n",
    "                print(f\"   Prediksi Model   : {row['prediksi'].upper()}\")\n",
    "                print(f\"   Analisis         : {row['analisis']}\")\n",
    "                print(\"-\" * 40)\n",
    "        pos_as_neg = misclassified_data[(misclassified_data['sentimen'] == 'positif') & (misclassified_data['prediksi'] == 'negatif')]\n",
    "        neg_as_pos = misclassified_data[(misclassified_data['sentimen'] == 'negatif') & (misclassified_data['prediksi'] == 'positif')]\n",
    "        sent_as_neu = misclassified_data[(misclassified_data['sentimen'] != 'netral') & (misclassified_data['prediksi'] == 'netral')]\n",
    "        neu_as_sent = misclassified_data[(misclassified_data['sentimen'] == 'netral') & (misclassified_data['prediksi'] != 'netral')]\n",
    "        print_group(\"Kesalahan Tipe 1: Sentimen POSITIF diprediksi NEGATIF\", pos_as_neg)\n",
    "        print_group(\"Kesalahan Tipe 2: Sentimen NEGATIF diprediksi POSITIF\", neg_as_pos)\n",
    "        print_group(\"Kesalahan Tipe 3: Sentimen Jelas (Pos/Neg) diprediksi NETRAL\", sent_as_neu)\n",
    "        print_group(\"Kesalahan Tipe 4: NETRAL diprediksi sebagai Sentimen Jelas\", neu_as_sent)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    def show_top_features(self, n=15):\n",
    "        try:\n",
    "            tfidf_vectorizer = self.pipeline.named_steps['tfidf']\n",
    "            classifier = self.pipeline.named_steps['classifier']\n",
    "            feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "            print(\"=\"*80)\n",
    "            print(f\"🔑 KATA KUNCI UTAMA PER SENTIMEN (TOP {n})\")\n",
    "            print(\"=\"*80)\n",
    "            for i, class_label in enumerate(classifier.classes_):\n",
    "                coefs = classifier.coef_[i]\n",
    "                top_coef_indices = coefs.argsort()[-n:][::-1]\n",
    "                top_words = feature_names[top_coef_indices]\n",
    "                top_coefs = coefs[top_coef_indices]\n",
    "                print(f\"\\n--- Sentimen: {class_label.upper()} ---\")\n",
    "                for word, coef in zip(top_words, top_coefs):\n",
    "                    print(f\"{word: <20} (Bobot: {coef:.3f})\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal menampilkan top features: {e}\")\n",
    "\n",
    "# SECTION 4: FUNGSI UTAMA UNTUK MENJALANKAN ALUR KERJA\n",
    "def run_workflow(file_path, sheet_names, text_column='Ulasan'):\n",
    "    print(\"=\"*80 + \"\\n🚀 ANALISIS SENTIMEN (V21 - Versi Stabil) 🚀\\n\" + \"=\"*80)\n",
    "    try:\n",
    "        all_df = [pd.read_excel(file_path, sheet_name=s, header=None, names=['Ulasan', 'Rating']) for s in sheet_names]\n",
    "        combined_df = pd.concat(all_df, ignore_index=True)\n",
    "        combined_df.dropna(subset=['Ulasan', 'Rating'], inplace=True)\n",
    "        combined_df['Rating'] = pd.to_numeric(combined_df['Rating'], errors='coerce')\n",
    "        combined_df.dropna(subset=['Rating'], inplace=True)\n",
    "        combined_df['Rating'] = combined_df['Rating'].astype(int)\n",
    "        \n",
    "        print(\"\\n🤖 Menerapkan pelabelan otomatis awal (berdasarkan rating)...\")\n",
    "        combined_df['sentimen'] = combined_df.apply(label_review_enhanced, axis=1)\n",
    "        print(\"Distribusi sentimen awal:\")\n",
    "        print(combined_df['sentimen'].value_counts())\n",
    "\n",
    "        print(\"\\n✨ Menyempurnakan pelabelan berbasis data...\")\n",
    "        temp_preprocessor = TextPreprocessor()\n",
    "        combined_df['cleaned_text'] = combined_df[text_column].apply(temp_preprocessor.preprocess_text)\n",
    "        combined_df['word_count_cleaned'] = combined_df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        strong_sentiment_words = analyze_word_sentiment_distribution(combined_df, threshold=0.70)\n",
    "        \n",
    "        condition_to_relabel = (combined_df['word_count_cleaned'] <= 1) & \\\n",
    "                               (~combined_df['cleaned_text'].isin(strong_sentiment_words))\n",
    "        \n",
    "        original_neutral_count = (combined_df['sentimen'] == 'netral').sum()\n",
    "        combined_df.loc[condition_to_relabel, 'sentimen'] = 'netral'\n",
    "        new_neutral_count = (combined_df['sentimen'] == 'netral').sum()\n",
    "        \n",
    "        print(f\"✅ Pelabelan ulang selesai. {new_neutral_count - original_neutral_count} ulasan diidentifikasi sebagai netral.\")\n",
    "\n",
    "        print(\"\\n📊 Distribusi sentimen final (setelah penyempurnaan):\")\n",
    "        print(combined_df['sentimen'].value_counts())\n",
    "\n",
    "        analyzer = SentimentAnalyzerLR()\n",
    "        analyzer.train_and_evaluate(combined_df, text_column='cleaned_text', target_column='sentimen') \n",
    "        \n",
    "        model_filename = 'jss_sentiment_model_v21.pkl'\n",
    "        print(f\"\\n💾 Menyimpan Model Final ke '{model_filename}'...\")\n",
    "        joblib.dump(analyzer, model_filename)\n",
    "        print(f\"   -> Model berhasil disimpan ke '{model_filename}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Terjadi kesalahan pada alur kerja utama: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# SECTION 5: TITIK MASUK EKSEKUSI PROGRAM\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_FILE_PATH = '/kaggle/input/jss-v5/Ulasan_JSSv5.xlsx'\n",
    "    SHEET_NAMES_TO_READ = ['Positif', 'Negatif', 'Netral']\n",
    "    \n",
    "    if os.path.exists(DATA_FILE_PATH):\n",
    "        run_workflow(file_path=DATA_FILE_PATH, sheet_names=SHEET_NAMES_TO_READ)\n",
    "    else:\n",
    "        try:\n",
    "            df_pos = pd.read_csv('Ulasan_JSS.xlsx - Positif.csv', header=None, names=['Ulasan', 'Rating'])\n",
    "            df_neg = pd.read_csv('Ulasan_JSS.xlsx - Negatif.csv', header=None, names=['Ulasan', 'Rating'])\n",
    "            df_net = pd.read_csv('Ulasan_JSS.xlsx - Netral.csv', header=None, names=['Ulasan', 'Rating'])\n",
    "            \n",
    "            with pd.ExcelWriter('Ulasan_JSS.xlsx') as writer:\n",
    "                df_pos.to_excel(writer, sheet_name='Positif', index=False, header=False)\n",
    "                df_neg.to_excel(writer, sheet_name='Negatif', index=False, header=False)\n",
    "                df_net.to_excel(writer, sheet_name='Netral', index=False, header=False)\n",
    "            \n",
    "            print(\"💡 File Excel tidak ditemukan, file CSV dibaca dan file Excel sementara dibuat.\")\n",
    "            run_workflow(file_path='Ulasan_JSS.xlsx', sheet_names=SHEET_NAMES_TO_READ)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ ERROR: File Excel maupun CSV tidak ditemukan.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7897032,
     "sourceId": 12511420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
